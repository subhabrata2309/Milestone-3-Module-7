{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47a6216a",
   "metadata": {},
   "source": [
    "Question 1: What is a Decision Tree, and how does it work in the context of\n",
    "classification?\n",
    "\n",
    "Answer:\n",
    "\n",
    "A Decision Tree is a flowchart-like model used for classification where data is split into branches based on feature conditions. Each internal node represents a test on a feature, branches represent outcomes, and leaf nodes represent final class labels. \n",
    "\n",
    "It works by recursively partitioning the dataset to maximize class separation.\n",
    "\n",
    "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
    "How do they impact the splits in a Decision Tree?\n",
    "\n",
    "Answer:\n",
    "\n",
    "Gini Impurity: Measures probability of incorrectly classifying a randomly chosen sample. (Lower Gini = purer node).\n",
    "Entropy: Measures disorder/uncertainty in a node. (Lower Entropy = higher purity).\n",
    "Impact: Decision Trees choose splits that minimize Gini/Entropy, leading to purer child nodes and better classification.\n",
    "\n",
    "\n",
    "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
    "Trees? Give one practical advantage of using each.\n",
    "\n",
    "Answer:\n",
    "\n",
    "Pre-Pruning: Stops tree growth early using conditions (e.g., max depth, min samples).\n",
    "Advantage: Saves time and prevents overfitting early.\n",
    "\n",
    "Post-Pruning: Grows full tree first, then removes weak branches.\n",
    "Advantage: Improves accuracy by simplifying the model after seeing complete structure.\n",
    "\n",
    "\n",
    "Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
    "choosing the best split?\n",
    "\n",
    "Answer:\n",
    "\n",
    "Information Gain measures the reduction in impurity (Entropy) after splitting a dataset on a feature.\n",
    "Importance: The feature with the highest Information Gain is chosen for the split, ensuring the tree separates classes most effectively.\n",
    "\n",
    "Question 5: What are some common real-world applications of Decision Trees, and\n",
    "what are their main advantages and limitations?\n",
    "\n",
    "Answer:\n",
    "\n",
    "Applications: Medical diagnosis, credit risk scoring, fraud detection, customer segmentation, recommendation systems.\n",
    "Advantages: Easy to understand, interpretable, handles categorical & numerical data.\n",
    "Limitations: Prone to overfitting, unstable with small data changes, less accurate than ensemble methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5503aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Feature Importances: [0.         0.01667014 0.90614339 0.07718647]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
    "provided CSV).\n",
    "● Boston Housing Dataset for regression tasks\n",
    "(sklearn.datasets.load_boston() or provided CSV).\n",
    "Question 6: Write a Python program to:\n",
    "● Load the Iris Dataset\n",
    "● Train a Decision Tree Classifier using the Gini criterion\n",
    "● Print the model’s accuracy and feature importances\n",
    "'''\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(\"Feature Importances:\", clf.feature_importances_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "039ccdc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Tree Accuracy: 1.0\n",
      "Max Depth=3 Tree Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Question 7: Write a Python program to:\n",
    "● Load the Iris Dataset\n",
    "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
    "a fully-grown tree.\n",
    "\n",
    "'''\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "full_tree = DecisionTreeClassifier(random_state=42)\n",
    "full_tree.fit(X_train, y_train)\n",
    "y_pred_full = full_tree.predict(X_test)\n",
    "print(\"Full Tree Accuracy:\", accuracy_score(y_test, y_pred_full))\n",
    "\n",
    "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "pruned_tree.fit(X_train, y_train)\n",
    "y_pred_pruned = pruned_tree.predict(X_test)\n",
    "print(\"Max Depth=3 Tree Accuracy:\", accuracy_score(y_test, y_pred_pruned))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0edbd785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.4979753095391472\n",
      "Feature Importances: [0.52760631 0.05182796 0.05397784 0.0284493  0.0302391  0.13141145\n",
      " 0.09337296 0.08311508]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Question 8: Write a Python program to:\n",
    "● Load the California Housing dataset from sklearn\n",
    "● Train a Decision Tree Regressor\n",
    "● Print the Mean Squared Error (MSE) and feature importances\n",
    "\n",
    "'''\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load California Housing dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Decision Tree Regressor\n",
    "reg = DecisionTreeRegressor(random_state=42)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and MSE\n",
    "y_pred = reg.predict(X_test)\n",
    "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Feature importances\n",
    "print(\"Feature Importances:\", reg.feature_importances_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3d112b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Question 9: Write a Python program to:\n",
    "● Load the Iris Dataset\n",
    "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
    "GridSearchCV\n",
    "● Print the best parameters and the resulting model accuracy\n",
    "'''\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5, None],\n",
    "    'min_samples_split': [2, 4, 6, 8, 10]}\n",
    "grid = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff94290",
   "metadata": {},
   "source": [
    "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
    "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
    "mixed data types and some missing values.\n",
    "Explain the step-by-step process you would follow to:\n",
    "● Handle the missing values\n",
    "● Encode the categorical features\n",
    "● Train a Decision Tree model\n",
    "● Tune its hyperparameters\n",
    "● Evaluate its performance\n",
    "And describe what business value this model could provide in the real-world\n",
    "setting.\n",
    "\n",
    "'''\n",
    "Answer :\n",
    "\n",
    "1. Handle missing values\n",
    "\n",
    "   a. Inspect pattern & % missing (MCAR/MAR/MNAR).\n",
    "   b. Small %: drop rows or simple impute (median for numeric, mode for categorical).\n",
    "   c. Larger or informative: add missing-indicator column + use KNN or IterativeImputer (MICE).\n",
    "   d. For MNAR use domain rules or model-based approaches.\n",
    "\n",
    "2. Encode categorical features\n",
    "\n",
    "   a. Identify ordinal vs nominal.\n",
    "   b. Ordinal → `OrdinalEncoder`; nominal (low-cardinality) → `OneHotEncoder`.\n",
    "   c. High-cardinality → frequency/target/hash encoding or embeddings.\n",
    "   d. Implement with `ColumnTransformer` inside a `Pipeline` to avoid leakage.\n",
    "\n",
    "3. Train Decision Tree\n",
    "\n",
    "   a. Split data stratified: train / validation / test.\n",
    "   b. Use a pipeline: preprocessing → `DecisionTreeClassifier` (scaling usually not required).\n",
    "   c. Handle imbalance: `class_weight='balanced'` or resample (SMOTE) inside CV.\n",
    "\n",
    "4. Tune hyperparameters\n",
    "\n",
    "   a. Use `GridSearchCV` or `RandomizedSearchCV` with stratified CV.\n",
    "   b. Tune `max_depth`, `min_samples_split`, `min_samples_leaf`, `max_features`, `criterion`, `class_weight`.\n",
    "   c. Optimize business-relevant metric (recall/ROC-AUC/PR-AUC), consider nested CV.\n",
    "\n",
    "5. Evaluate performance\n",
    "\n",
    "   a. On test set report: confusion matrix, recall (sensitivity), precision, F1, ROC-AUC, PR-AUC, calibration (Brier).\n",
    "   b. Tune decision threshold for business trade-offs (FP vs FN cost).\n",
    "   c. Explain with feature importances + SHAP; check fairness and clinical plausibility.\n",
    "\n",
    "6. Deployment & monitoring (brief)**\n",
    "\n",
    "   a. Clinician validation, integrate into workflow, log predictions, monitor drift, retrain periodically, ensure privacy/compliance.\n",
    "\n",
    "7. Business value\n",
    "\n",
    "   a. Early detection and triage, prioritize tests/resources, reduce costs, improve patient outcomes, enable targeted interventions and measurable ROI — provided thresholds and clinical validation align with real-world cost/benefit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0891e703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
